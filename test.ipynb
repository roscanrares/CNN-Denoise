{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "#from model import PRIDNet  # Assuming PRIDNet is defined in model.py\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttentionModule(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ChannelAttentionModule, self).__init__()\n",
    "    self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "    self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Conv2d(2, 1, 1, bias=False),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    avg_out = self.avg_pool(x)\n",
    "    max_out = self.max_pool(x)\n",
    "    out = torch.cat([avg_out, max_out], dim=1)\n",
    "    out = self.fc(out)\n",
    "    return x * out\n",
    "\n",
    "class KernelSelectingModule(nn.Module):\n",
    "  def __init__(self, in_channels, channels):\n",
    "    super(KernelSelectingModule, self).__init__()\n",
    "    self.conv = nn.Conv2d(in_channels, channels, kernel_size=1, bias=False)\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv(x)\n",
    "    out = self.softmax(out)\n",
    "    return out\n",
    "\n",
    "class PRIDNet(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels):\n",
    "    super(PRIDNet, self).__init__()\n",
    "\n",
    "    # U-Net\n",
    "    self.unet = nn.Sequential(\n",
    "      # Encoder\n",
    "      nn.Conv2d(in_channels, 32, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "      nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "      nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "      nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "\n",
    "      # Decoder\n",
    "      nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "      nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "      nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "      nn.ConvTranspose2d(32, out_channels, kernel_size=2, stride=2),\n",
    "      nn.ReLU(inplace=True),\n",
    "      nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "    )\n",
    "\n",
    "    # Channel Attention Module\n",
    "    self.cam = ChannelAttentionModule()\n",
    "\n",
    "    # Kernel Selecting Module\n",
    "    self.ksm = KernelSelectingModule(32, 8)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.unet(x)\n",
    "\n",
    "    # Kernel Selection\n",
    "    # 1. Extract bottleneck features from the encoder (modify if needed)\n",
    "    bottleneck = out  # Assuming bottleneck features are at the end of the encoder\n",
    "\n",
    "    # 2. Apply Kernel Selecting Module\n",
    "    kernels = self.ksm(bottleneck)  # Get weights for each kernel\n",
    "\n",
    "    # 3. Expand kernels (assuming kernels are defined elsewhere)\n",
    "    expanded_kernels = []  # Placeholder for expanded kernels\n",
    "    for i in range(len(kernels[0])):  # Iterate through kernel weights\n",
    "      # Assuming you have pre-defined kernels (kernel1, kernel2, ...)\n",
    "      # Expand the weights from a 1x1 kernel to match the actual kernel size\n",
    "      expanded_kernels.append(nn.functional.conv2d(kernel[i].unsqueeze(1), weight=torch.ones(1, 1, *kernel_size).cuda()))\n",
    "\n",
    "    # 4. Apply weighted kernels (replace with your desired operation)\n",
    "    weighted_features = torch.zeros_like(out)  # Placeholder for weighted features\n",
    "    for i in range(len(expanded_kernels)):\n",
    "      weighted_features += kernels[:, i, 0, 0].unsqueeze(1) * nn.functional.conv2d(out, expanded_kernels[i])\n",
    "\n",
    "    # 5. Combine with original features (replace with your desired operation)\n",
    "    out = self.cam(weighted_features + out)  # Apply CAM and combine\n",
    "\n",
    "    # Rest of the decoder (same as before)\n",
    "    # ... your decoder architecture here ...\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(denoised, target):\n",
    "  \"\"\"\n",
    "  Calculates Peak Signal-to-Noise Ratio (PSNR) between two images.\n",
    "  Args:\n",
    "    denoised: Denoised image tensor.\n",
    "    target: Clean image tensor.\n",
    "  Returns:\n",
    "    PSNR value in dB.\n",
    "  \"\"\"\n",
    "  mse = nn.functional.mse_loss(denoised, target)\n",
    "  max_pixel = 1.0  # Assuming pixel values are normalized between 0 and 1\n",
    "  psnr = 10 * torch.log10(max_pixel**2 / mse)\n",
    "  return psnr.item()\n",
    "\n",
    "def load_data(noisy_dir, good_dir, size=128):\n",
    "  \"\"\"\n",
    "  Loads and preprocesses noisy and clean image data from directories.\n",
    "  Args:\n",
    "    noisy_dir: Path to the directory containing noisy images.\n",
    "    good_dir: Path to the directory containing clean (good) images.\n",
    "    size: Target size for image resizing (default: 128).\n",
    "  Returns:\n",
    "    A list of tuples (noisy_image, clean_image).\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(size),  # Resize to desired size\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [0, 1]\n",
    "  ])\n",
    "  for filename in os.listdir(noisy_dir):\n",
    "    noisy_path = os.path.join(noisy_dir, filename)\n",
    "    clean_path = os.path.join(good_dir, filename)\n",
    "    if os.path.isfile(noisy_path) and os.path.isfile(clean_path):\n",
    "      noisy_image = transform(Image.open(noisy_path).resize((size, size)).convert('RGB'))\n",
    "      clean_image = transform(Image.open(clean_path).resize((size, size)).convert('RGB'))\n",
    "      data.append((noisy_image, clean_image))\n",
    "    if (len(data) >= 50):\n",
    "      break\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"data\\\\train_noisy\", \"data\\\\train\", 256)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 256, 256]) torch.Size([4, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for n, c in train_loader:\n",
    "    print(n.shape, c.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [8, 32, 1, 1], expected input[4, 3, 512, 512] to have 32 channels, but got 3 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m noisy_image, clean_image \u001b[38;5;241m=\u001b[39m noisy_image\u001b[38;5;241m.\u001b[39mto(device), clean_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m denoised_image \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(denoised_image, clean_image)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Backward pass and update weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 84\u001b[0m, in \u001b[0;36mPRIDNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m bottleneck \u001b[38;5;241m=\u001b[39m out  \u001b[38;5;66;03m# Assuming bottleneck features are at the end of the encoder\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# 2. Apply Kernel Selecting Module\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m kernels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mksm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbottleneck\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Get weights for each kernel\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# 3. Expand kernels (assuming kernels are defined elsewhere)\u001b[39;00m\n\u001b[0;32m     87\u001b[0m expanded_kernels \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Placeholder for expanded kernels\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m, in \u001b[0;36mKernelSelectingModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 25\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(out)\n\u001b[0;32m     27\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [8, 32, 1, 1], expected input[4, 3, 512, 512] to have 32 channels, but got 3 channels instead"
     ]
    }
   ],
   "source": [
    "def train_model(epochs, learning_rate, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "  \"\"\"\n",
    "  Trains the PRIDNet model for denoising images.\n",
    "  Args:\n",
    "    epochs: Number of training epochs.\n",
    "    learning_rate: Learning rate for the optimizer.\n",
    "    noisy_dir: Path to the directory containing noisy images.\n",
    "    good_dir: Path to the directory containing clean (good) images.\n",
    "    device: Device to use for training (CPU or GPU).\n",
    "  \"\"\"\n",
    "\n",
    "  # Model and optimizer\n",
    "  model = PRIDNet(in_channels=3, out_channels=3)  # Assuming 3 channels for RGB\n",
    "  model.to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  criterion = nn.MSELoss()\n",
    "\n",
    "  # Training loop\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for noisy_image, clean_image in train_loader:\n",
    "      noisy_image, clean_image = noisy_image.to(device), clean_image.to(device)\n",
    "\n",
    "      # Forward pass\n",
    "      denoised_image = model(noisy_image)\n",
    "      loss = criterion(denoised_image, clean_image)\n",
    "\n",
    "      # Backward pass and update weights\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      # Optional: Calculate validation PSNR on a separate validation set\n",
    "      # ... your validation code here ...\n",
    "      pass\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{epochs} | Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Example usage\n",
    "train_model(epochs=100, learning_rate=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
